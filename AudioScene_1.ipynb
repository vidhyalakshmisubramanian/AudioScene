{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmQOdn-wl7F0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = \"/content/drive/MyDrive/COCO\"\n",
        "IMG_PATH = f\"{BASE_PATH}/train2017\"\n",
        "ANN_PATH = f\"{BASE_PATH}/annotations/captions_train2017.json\""
      ],
      "metadata": {
        "id": "8kMJZixOmRIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ANN_PATH = \"/content/annotations/captions_train2017.json\""
      ],
      "metadata": {
        "id": "ba1hcRNCoioN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists(ANN_PATH))  # must be True"
      ],
      "metadata": {
        "id": "i4rz21L7ol3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(ANN_PATH, \"r\") as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "captions = []\n",
        "\n",
        "for ann in coco_data[\"annotations\"]:\n",
        "    cap = ann[\"caption\"].lower().strip()\n",
        "    cap = \"<start> \" + cap + \" <end>\"\n",
        "    captions.append(cap)"
      ],
      "metadata": {
        "id": "erz0J7LdorRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total captions:\", len(captions))\n",
        "print(\"Sample:\", captions[0])"
      ],
      "metadata": {
        "id": "GJcICuF4ovDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN** **ENCODER**"
      ],
      "metadata": {
        "id": "0cE2xYtEmjgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "base_model = InceptionV3(weights='imagenet')\n",
        "base_model.trainable = False\n",
        "\n",
        "encoder = Model(\n",
        "    base_model.input,\n",
        "    base_model.layers[-2].output\n",
        ")"
      ],
      "metadata": {
        "id": "cLO0557Smc8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bahdanau Attention**"
      ],
      "metadata": {
        "id": "ZUlw3Smgmrvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "BmlkdCZbm5V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        \"\"\"\n",
        "        features: (batch_size, num_features, embedding_dim)\n",
        "        hidden:   (batch_size, units)\n",
        "        \"\"\"\n",
        "\n",
        "        # Expand hidden state for addition\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # Score calculation\n",
        "        score = tf.nn.tanh(\n",
        "            self.W1(features) + self.W2(hidden_with_time_axis)\n",
        "        )\n",
        "\n",
        "        # Attention weights\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "\n",
        "        # Context vector\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "90I6055VpCPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNDecoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, units):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # ðŸ”‘ IMPORTANT: return_state = False\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            self.units,\n",
        "            return_sequences=True,\n",
        "            recurrent_initializer=\"glorot_uniform\"\n",
        "        )\n",
        "\n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        # Attention\n",
        "        context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "        # Embedding\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Concatenate context + embedding\n",
        "        x = tf.concat(\n",
        "            [tf.expand_dims(context_vector, 1), x],\n",
        "            axis=-1\n",
        "        )\n",
        "\n",
        "        # ðŸ”‘ GRU RETURNS ONLY ONE TENSOR NOW\n",
        "        output = self.gru(x, initial_state=hidden)\n",
        "\n",
        "        # Use last timestep as new hidden state\n",
        "        state = output[:, -1, :]\n",
        "\n",
        "        # Fully connected\n",
        "        x = self.fc1(output)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x, state, attention_weights\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))\n"
      ],
      "metadata": {
        "id": "z1Flg9IDIrLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "import numpy as np\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = preprocess_input(img)\n",
        "    return img"
      ],
      "metadata": {
        "id": "-NBJrsqtMmty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(ANN_PATH, \"r\") as f:\n",
        "    coco = json.load(f)"
      ],
      "metadata": {
        "id": "qdk_0-_5MoHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_id_to_filename = {\n",
        "    img[\"id\"]: img[\"file_name\"]\n",
        "    for img in coco[\"images\"]\n",
        "}"
      ],
      "metadata": {
        "id": "G64T3nxJM1OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann = coco[\"annotations\"][0]\n",
        "\n",
        "image_id = ann[\"image_id\"]\n",
        "filename = image_id_to_filename[image_id]\n",
        "\n",
        "image_path = f\"{IMG_PATH}/{filename}\"\n",
        "print(image_path)"
      ],
      "metadata": {
        "id": "-f70YW3TM3AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_captions = []\n",
        "image_paths = []\n",
        "\n",
        "for ann in coco[\"annotations\"]:\n",
        "    img_id = ann[\"image_id\"]\n",
        "    filename = image_id_to_filename[img_id]\n",
        "\n",
        "    image_paths.append(f\"{IMG_PATH}/{filename}\")\n",
        "    raw_captions.append(\n",
        "        \"startseq \" + ann[\"caption\"].lower().strip() + \" endseq\"\n",
        "    )"
      ],
      "metadata": {
        "id": "VFDGOAxjMcHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(oov_token=\"unk\")\n",
        "tokenizer.fit_on_texts(raw_captions)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "metadata": {
        "id": "zWNyRYSrLh8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 256\n",
        "units = 512\n",
        "\n",
        "decoder = RNNDecoder(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    units=units\n",
        ")"
      ],
      "metadata": {
        "id": "05-G6f8EpI3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert \"tokenizer\" in globals()\n",
        "assert \"vocab_size\" in globals()\n",
        "print(\"Tokenizer & vocab_size are ready\")"
      ],
      "metadata": {
        "id": "_a6oU6AfpSrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN Encoder (Image â†’ Feature Map)**"
      ],
      "metadata": {
        "id": "HfS7-iMppfVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "DbClGx_7pZB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Pretrained CNN**"
      ],
      "metadata": {
        "id": "RBPKviRWpp9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = InceptionV3(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False\n",
        ")\n",
        "\n",
        "base_model.trainable = False"
      ],
      "metadata": {
        "id": "nAlwiejxpmst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build encoder model**"
      ],
      "metadata": {
        "id": "_pM4WQLopwXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Model(\n",
        "    inputs=base_model.input,\n",
        "    outputs=base_model.output\n",
        ")"
      ],
      "metadata": {
        "id": "T7eUqTxGptmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image Preprocessing**"
      ],
      "metadata": {
        "id": "e6wWQC2ap3bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "import numpy as np\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = preprocess_input(img)\n",
        "    return img"
      ],
      "metadata": {
        "id": "sYUAHD1Dp04X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/COCO"
      ],
      "metadata": {
        "id": "gZRfDKFZstYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/COCO\n",
        "%cd /content/drive/MyDrive/COCO"
      ],
      "metadata": {
        "id": "QJlTH6iEsuuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/zips/train2017.zip\n",
        "!unzip train2017.zip"
      ],
      "metadata": {
        "id": "su2wZJF2sxDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir(\"/content/drive/MyDrive/COCO/train2017\"))"
      ],
      "metadata": {
        "id": "08p4Zb29yq_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = []\n",
        "caption_sequences = []\n",
        "\n",
        "for ann in coco[\"annotations\"]:\n",
        "    image_id = ann[\"image_id\"]\n",
        "    filename = image_id_to_filename[image_id]\n",
        "    img_path = f\"{IMG_PATH}/{filename}\"\n",
        "\n",
        "    caption = \"<start> \" + ann[\"caption\"].lower().strip() + \" <end>\"\n",
        "    seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "\n",
        "    image_paths.append(img_path)\n",
        "    caption_sequences.append(seq)"
      ],
      "metadata": {
        "id": "40UjGBta3PC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max(len(seq) for seq in caption_sequences)"
      ],
      "metadata": {
        "id": "wMGKM0S83_Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = []\n",
        "caption_sequences = []\n",
        "\n",
        "for ann in coco[\"annotations\"]:\n",
        "    caption = ann[\"caption\"].lower().strip()\n",
        "    caption = \"<start> \" + caption + \" <end>\"\n",
        "\n",
        "    seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "\n",
        "    # ðŸ”´ IMPORTANT FILTER\n",
        "    if len(seq) < 2:\n",
        "        continue\n",
        "\n",
        "    image_id = ann[\"image_id\"]\n",
        "    filename = image_id_to_filename[image_id]\n",
        "    img_path = f\"{IMG_PATH}/{filename}\"\n",
        "\n",
        "    image_paths.append(img_path)\n",
        "    caption_sequences.append(seq)"
      ],
      "metadata": {
        "id": "QXGDA40x4RSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear contaminated variables\n",
        "del caption_sequences\n",
        "del image_paths"
      ],
      "metadata": {
        "id": "iCu5KAkL5oXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = []\n",
        "caption_sequences = []\n",
        "\n",
        "for ann in coco[\"annotations\"]:\n",
        "    caption = ann[\"caption\"].lower().strip()\n",
        "    caption = \"<start> \" + caption + \" <end>\"\n",
        "\n",
        "    seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "\n",
        "    # STRICT FILTER\n",
        "    if not seq or len(seq) < 2:\n",
        "        continue\n",
        "\n",
        "    image_id = ann[\"image_id\"]\n",
        "    filename = image_id_to_filename[image_id]\n",
        "    img_path = f\"{IMG_PATH}/{filename}\"\n",
        "\n",
        "    image_paths.append(img_path)\n",
        "    caption_sequences.append(seq)"
      ],
      "metadata": {
        "id": "PFHaXlav5uF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import json, os\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "6uDRTyPw6ORN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = \"/content/drive/MyDrive/COCO\"\n",
        "IMG_PATH = f\"{BASE_PATH}/train2017\"\n",
        "ANN_PATH = f\"{BASE_PATH}/annotations/captions_train2017.json\""
      ],
      "metadata": {
        "id": "0gZjSifh6Q9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(ANN_PATH, \"r\") as f:\n",
        "    coco = json.load(f)"
      ],
      "metadata": {
        "id": "YAAe9RKF6TZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_length = max(len(seq) for seq in caption_sequences)\n",
        "\n",
        "caption_sequences = pad_sequences(\n",
        "    caption_sequences,\n",
        "    maxlen=max_length,\n",
        "    padding=\"post\",\n",
        "    truncating=\"post\"\n",
        ")"
      ],
      "metadata": {
        "id": "aOVobJcb5-6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_sequences = []\n",
        "clean_image_paths = []\n",
        "\n",
        "for seq, img_path in zip(caption_sequences, image_paths):\n",
        "    if seq is None:\n",
        "        continue\n",
        "    if len(seq) < 2:\n",
        "        continue\n",
        "    if any(x is None for x in seq):\n",
        "        continue\n",
        "\n",
        "    clean_sequences.append(seq)\n",
        "    clean_image_paths.append(img_path)\n",
        "\n",
        "caption_sequences = clean_sequences\n",
        "image_paths = clean_image_paths"
      ],
      "metadata": {
        "id": "FwxtOPt2ODFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# ðŸ”‘ Force all sequences to be clean Python lists of ints\n",
        "clean_sequences = []\n",
        "\n",
        "for seq in caption_sequences:\n",
        "    if seq is None:\n",
        "        continue\n",
        "    if len(seq) < 2:\n",
        "        continue\n",
        "    clean_sequences.append([int(x) for x in seq])\n",
        "\n",
        "caption_sequences = clean_sequences\n",
        "\n",
        "# Compute max length safely\n",
        "max_length = max(len(seq) for seq in caption_sequences)\n",
        "\n",
        "# Pad\n",
        "caption_sequences = pad_sequences(\n",
        "    caption_sequences,\n",
        "    maxlen=max_length,\n",
        "    padding=\"post\",\n",
        "    truncating=\"post\",\n",
        "    dtype=\"int32\"\n",
        ")"
      ],
      "metadata": {
        "id": "XW6JaU2x411P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(image_paths))\n",
        "print(len(caption_sequences))\n",
        "print(type(caption_sequences))"
      ],
      "metadata": {
        "id": "9XLfOn-I4kJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.path.exists(\"/content/drive/MyDrive/COCO\"))\n",
        "print(os.path.exists(\"/content/drive/MyDrive/COCO/annotations\"))\n",
        "print(os.path.exists(\"/content/drive/MyDrive/COCO/annotations/captions_train2017.json\"))"
      ],
      "metadata": {
        "id": "-zK7NGIe6jFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/COCO"
      ],
      "metadata": {
        "id": "YKQHcTnI7Aup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
      ],
      "metadata": {
        "id": "5BUAvrhd7qhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip annotations_trainval2017.zip"
      ],
      "metadata": {
        "id": "SsJ-oPba7-gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.path.exists(\"/content/drive/MyDrive/COCO/annotations\"))\n",
        "print(os.path.exists(\"/content/drive/MyDrive/COCO/annotations/captions_train2017.json\"))"
      ],
      "metadata": {
        "id": "TgZiwoO075_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = \"/content/drive/MyDrive/COCO\"\n",
        "IMG_PATH = f\"{BASE_PATH}/train2017\"\n",
        "ANN_PATH = f\"{BASE_PATH}/annotations/captions_train2017.json\""
      ],
      "metadata": {
        "id": "8tmyBCwa8Z0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(ANN_PATH, \"r\") as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "print(\"Images:\", len(coco[\"images\"]))\n",
        "print(\"Captions:\", len(coco[\"annotations\"]))"
      ],
      "metadata": {
        "id": "qqtbWJhc8lWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_id_to_filename = {\n",
        "    img[\"id\"]: img[\"file_name\"]\n",
        "    for img in coco[\"images\"]\n",
        "}"
      ],
      "metadata": {
        "id": "hCZeHT0l8sTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_captions = []\n",
        "raw_image_paths = []\n",
        "\n",
        "for ann in coco[\"annotations\"]:\n",
        "    image_id = ann[\"image_id\"]\n",
        "    filename = image_id_to_filename[image_id]\n",
        "    img_path = f\"{IMG_PATH}/{filename}\"\n",
        "\n",
        "    caption = \"startseq \" + ann[\"caption\"].lower().strip() + \" endseq\"\n",
        "\n",
        "\n",
        "    raw_captions.append(caption)\n",
        "    raw_image_paths.append(img_path)\n",
        "\n",
        "print(len(raw_captions), len(raw_image_paths))"
      ],
      "metadata": {
        "id": "YkAlUHCr8ul1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(oov_token=\"unk\")\n",
        "tokenizer.fit_on_texts(raw_captions)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "metadata": {
        "id": "qnZd-8jD-xcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_sequences = tokenizer.texts_to_sequences(raw_captions)"
      ],
      "metadata": {
        "id": "vlXYZ1TR82O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_sequences = []\n",
        "clean_image_paths = []\n",
        "\n",
        "for seq, img_path in zip(caption_sequences, image_paths):\n",
        "    if not seq:\n",
        "        continue\n",
        "    if len(seq) < 2:\n",
        "        continue\n",
        "\n",
        "    clean_sequences.append(seq)\n",
        "    clean_image_paths.append(img_path)\n",
        "\n",
        "caption_sequences = clean_sequences\n",
        "image_paths = clean_image_paths\n",
        "\n",
        "print(len(caption_sequences), len(image_paths))"
      ],
      "metadata": {
        "id": "-EGbm1Cz9IRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_length = max(len(seq) for seq in caption_sequences)\n",
        "\n",
        "caption_sequences = pad_sequences(\n",
        "    caption_sequences,\n",
        "    maxlen=max_length,\n",
        "    padding=\"post\",\n",
        "    truncating=\"post\",\n",
        "    dtype=\"int32\"\n",
        ")"
      ],
      "metadata": {
        "id": "afluHf-v9W8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(caption_sequences))\n",
        "print(caption_sequences.shape)\n",
        "print(caption_sequences.dtype)"
      ],
      "metadata": {
        "id": "pUsGLP2D9Z_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "def load_image_tf(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = preprocess_input(img)\n",
        "    return img"
      ],
      "metadata": {
        "id": "7yqDAyNq9iji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_func(img_path, caption):\n",
        "    img = load_image_tf(img_path)\n",
        "    return img, caption"
      ],
      "metadata": {
        "id": "Mi03KUvz9neW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "def load_image_tf(img_path):\n",
        "    # Read image file\n",
        "    img = tf.io.read_file(img_path)\n",
        "\n",
        "    # Decode JPEG\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "\n",
        "    # Resize for InceptionV3\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "\n",
        "    # Preprocess (scale pixels as InceptionV3 expects)\n",
        "    img = preprocess_input(img)\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "lyXwXcJePtBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (image_paths, caption_sequences)\n",
        ")\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.map(\n",
        "    lambda x, y: (load_image_tf(x), y),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "kbvqGBfj9o8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, cap = next(iter(dataset))\n",
        "print(img.shape, cap.shape)"
      ],
      "metadata": {
        "id": "eweajlYzPylo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction=\"none\"\n",
        ")\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    # Mask padding (0)\n",
        "    mask = tf.math.not_equal(real, 0)\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n"
      ],
      "metadata": {
        "id": "HCPgI6MlTt4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "metadata": {
        "id": "SD0FU21I9wkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('start' in tokenizer.word_index)\n",
        "print('<start>' in tokenizer.word_index)"
      ],
      "metadata": {
        "id": "jr7oYESX-Sx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_loss = tf.constant(0.0)"
      ],
      "metadata": {
        "id": "kvz2qz0DCNzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOTAL_SPLITS = 5\n",
        "BATCHES_PER_SPLIT = 1850 // TOTAL_SPLITS  # â‰ˆ 370"
      ],
      "metadata": {
        "id": "o5GdJSprAO_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"âœ… GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(\"âŒ Error enabling memory growth:\", e)\n",
        "else:\n",
        "    print(\"âš ï¸ No GPU found\")"
      ],
      "metadata": {
        "id": "2Pmx9P-RHOpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.experimental.get_memory_growth(gpus[0])\n",
        "print('yes')"
      ],
      "metadata": {
        "id": "cbc39jHPHXv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FORCE overwrite train_step (no autograph, no cache)\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0.0\n",
        "\n",
        "    batch_size = tf.shape(target)[0]\n",
        "    hidden = decoder.reset_state(batch_size=batch_size)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = encoder(img_tensor)  # NO reshape here\n",
        "\n",
        "        dec_input = tf.expand_dims(\n",
        "            tf.fill([batch_size], tokenizer.word_index['startseq']),\n",
        "            1\n",
        "        )\n",
        "\n",
        "        for t in range(1, max_length):\n",
        "            predictions, hidden, _ = decoder(\n",
        "                dec_input, features, hidden\n",
        "            )\n",
        "\n",
        "            loss += loss_function(target[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(target[:, t], 1)\n",
        "\n",
        "    total_loss = loss / tf.cast(max_length, tf.float32)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(total_loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "GIzspy8STNMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "print(inspect.getsource(train_step))"
      ],
      "metadata": {
        "id": "Mog-GvW0TQEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for v in decoder.trainable_variables:\n",
        "    print(v.name)"
      ],
      "metadata": {
        "id": "2adetRymT1UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(img_tensor, target):\n",
        "    loss = tf.constant(0.0)\n",
        "\n",
        "    batch_size = tf.shape(target)[0]\n",
        "    hidden = decoder.reset_state(batch_size=batch_size)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = encoder(img_tensor)\n",
        "\n",
        "        dec_input = tf.expand_dims(\n",
        "            tf.fill([batch_size], tokenizer.word_index['startseq']),\n",
        "            1\n",
        "        )\n",
        "\n",
        "        for t in range(1, max_length):\n",
        "            predictions, hidden, _ = decoder(\n",
        "                dec_input, features, hidden\n",
        "            )\n",
        "\n",
        "            loss += loss_function(target[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(target[:, t], 1)\n",
        "\n",
        "    total_loss = loss / tf.cast(max_length, tf.float32)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(total_loss, variables)"
      ],
      "metadata": {
        "id": "k489xSXIUGjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, cap = next(iter(dataset))\n",
        "print(train_step(img, cap))"
      ],
      "metadata": {
        "id": "3J1tj-S_TUSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(image_paths))\n",
        "print(type(image_paths[0]))\n",
        "print(image_paths[0])"
      ],
      "metadata": {
        "id": "EGtrBXvhVQC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (image_paths, caption_sequences)\n",
        ")\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.map(\n",
        "    lambda x, y: (load_image_tf(x), y),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"âœ… Dataset created\")"
      ],
      "metadata": {
        "id": "trDPK_SSU3au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCHES_PER_SPLIT = 400   # ~6â€“8 minutes\n",
        "TOTAL_SPLITS = 5         # ~30â€“40 minutes total"
      ],
      "metadata": {
        "id": "HU60CT4YdRNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/AudioScene_autosave\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "SAVE_INTERVAL = 180  # seconds (3 minutes)\n",
        "\n",
        "total_loss = 0.0\n",
        "step = 0\n",
        "save_count = 0\n",
        "\n",
        "start_time = time.time()\n",
        "last_save_time = start_time\n",
        "\n",
        "for img, cap in dataset:\n",
        "    batch_loss = train_step(img, cap)\n",
        "\n",
        "    # ðŸ”’ Safety check\n",
        "    if batch_loss is None:\n",
        "        print(f\"âš ï¸ Skipping batch at step {step}\")\n",
        "        continue\n",
        "\n",
        "    total_loss += batch_loss.numpy()\n",
        "    step += 1\n",
        "\n",
        "    # Logging\n",
        "    if step % 50 == 0:\n",
        "        print(f\"Step {step} | Loss {batch_loss.numpy():.4f}\")\n",
        "\n",
        "    # â±ï¸ AUTO-SAVE EVERY 3 MINUTES\n",
        "    current_time = time.time()\n",
        "    if current_time - last_save_time >= SAVE_INTERVAL:\n",
        "        save_count += 1\n",
        "\n",
        "        encoder.save_weights(\n",
        "            f\"{SAVE_DIR}/encoder_autosave_{save_count}.weights.h5\"\n",
        "        )\n",
        "        decoder.save_weights(\n",
        "            f\"{SAVE_DIR}/decoder_autosave_{save_count}.weights.h5\"\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"\\nðŸ’¾ Auto-saved checkpoint {save_count} \"\n",
        "            f\"at step {step} \"\n",
        "            f\"(elapsed {int(current_time - start_time)} sec)\\n\"\n",
        "        )\n",
        "\n",
        "        last_save_time = current_time"
      ],
      "metadata": {
        "id": "PWY96yt5USJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_id = 2  # any number you want to label this checkpoint\n",
        "\n",
        "encoder.save_weights(\n",
        "    f\"/content/drive/MyDrive/encoder_split_{split_id}.weights.h5\"\n",
        ")\n",
        "decoder.save_weights(\n",
        "    f\"/content/drive/MyDrive/decoder_split_{split_id}.weights.h5\"\n",
        ")\n",
        "\n",
        "print(\"ðŸ’¾ Checkpoint saved successfully\")"
      ],
      "metadata": {
        "id": "agtbY5_3fI4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.load_weights(\"/content/drive/MyDrive/encoder_split_1.weights.h5\")\n",
        "decoder.load_weights(\"/content/drive/MyDrive/decoder_split_1.weights.h5\")\n",
        "\n",
        "print(\"âœ… Weights loaded for continued training\")"
      ],
      "metadata": {
        "id": "S7aTr8_Xh_T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists(f\"/content/drive/MyDrive/encoder_split_{split_id}.weights.h5\"))\n",
        "print(os.path.exists(f\"/content/drive/MyDrive/decoder_split_{split_id}.weights.h5\"))"
      ],
      "metadata": {
        "id": "dJcQIJV6fS6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.load_weights(\"/content/drive/MyDrive/encoder_split_1.weights.h5\")\n",
        "decoder.load_weights(\"/content/drive/MyDrive/decoder_split_1.weights.h5\")\n",
        "\n",
        "print(\"âœ… Trained weights loaded\")"
      ],
      "metadata": {
        "id": "hRm5GuZBfvIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "ccAeSTjtsSQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate"
      ],
      "metadata": {
        "id": "fbh3_1GZp6aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration"
      ],
      "metadata": {
        "id": "l4gyaG0kp_co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\n",
        "    \"Salesforce/blip-image-captioning-base\"\n",
        ")\n",
        "\n",
        "model = BlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip-image-captioning-base\"\n",
        ").to(device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(\"âœ… BLIP model loaded\")"
      ],
      "metadata": {
        "id": "_AN6Iqq7qCuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = list(uploaded.keys())[0]\n",
        "print(\"ðŸ“· Uploaded image:\", image_path)"
      ],
      "metadata": {
        "id": "Chccw6Vvsjok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def generate_caption_blip(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(\n",
        "        image,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=40\n",
        "        )\n",
        "\n",
        "    caption = processor.decode(\n",
        "        output_ids[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return caption"
      ],
      "metadata": {
        "id": "6O0MbKloqYkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_image_with_blip_caption(image_path):\n",
        "    caption = generate_caption_blip(image_path)\n",
        "\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(caption, fontsize=12, wrap=True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Jt9MwK-vqcri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gTTS"
      ],
      "metadata": {
        "id": "44VhOIGirG6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "from IPython.display import Audio, display\n",
        "import uuid"
      ],
      "metadata": {
        "id": "NGiW22ALreB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def speak_caption(text):\n",
        "    \"\"\"\n",
        "    Converts caption text to speech and plays it.\n",
        "    \"\"\"\n",
        "    filename = f\"/content/{uuid.uuid4()}.mp3\"\n",
        "\n",
        "    tts = gTTS(text=text, lang=\"en\", slow=False)\n",
        "    tts.save(filename)\n",
        "\n",
        "    display(Audio(filename, autoplay=True))"
      ],
      "metadata": {
        "id": "TYwJzZA_rioz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image_caption_and_audio(image_path):\n",
        "    caption = generate_caption_blip(image_path)\n",
        "\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(caption, fontsize=12, wrap=True)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"ðŸ”Š Speaking caption:\")\n",
        "    speak_caption(caption)"
      ],
      "metadata": {
        "id": "nJB_TagyrmTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image_caption_and_audio(image_path)"
      ],
      "metadata": {
        "id": "s_dgrQo1snlH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}